%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for STAT 547C Final Project Outline
% Author: Ben Bloem-Reddy <benbr@stat.ubc.ca>
% Date: Oct. 17, 2019
% Acknowledgments: ETH, Peter Orbanz, John Cunningham
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[]{STAT_547C}
\usepackage{STAT_547C}
% NOTE: change the name and email address to your name in STAT_547C.sty

\usepackage{booktabs}
\usepackage{amsmath,amsthm,amssymb,amsfonts}

\usepackage[sorting=none,backend=biber,bibstyle=alphabetic,citestyle=alphabetic,giveninits=true,natbib=true]{biblatex}
\bibliography{/home/sean/Projects/sl-stat547c/ref/STAT_547C.bib} % add the title and location of your bibliography file

\begin{document}

% NOTE: You will replace the title below with your actual Title.
\makeGenericHeader{Probabilistic symmetry and convolutional neural networks}{Project Outline}
\vspace{-2cm}


%%%%%%%%%%%%%%%%%%%
\section{Title}

The working title of my project is \emph{Probabilistic symmetry and convolutional neural networks}.  

%%%%%%%%%%%%%%%%%%%
\section{Background}

Deep learning has revolutionized the research field of machine learning due to its powerful accuracy and generalizability on large, high dimensional datasets.
The convolutional neural network (CNNs) architecture has become the mainstay for constructing computer vision platforms.
The chief design principle of CNNs is the incorporation of convolutional layers that allow the network to detect certain features in the input image.
In particular, these convolutional layers are equivariant under translation of the image, in the sense that translations of the input image cause the activations of the convolutional layers to translate in the same way.
This design feature is the key reason why convolutional neural networks have achieved excellent generalizability on a variety of datasets.

In the last decade, there has been much work in expanding mathematical understanding of deep learning algorithms.
Deep learning algorithms can be formalized under two separate (but not mutually exclusive) points of views: they can be viewed as deterministic functions $f: \mathcal{X} \to \mathcal{Y}$ where $\mathcal{X}$ is the set of input features and $\mathcal{Y}$ is the set of labels, or they can be viewed from a probabilistic point of view so that $Y = f(X, \eta)$, where $X$ and $Y$ are the input and output random variables, and $\eta$ is some outsourced noise random variable.

Viewing deep learning algorithms deterministically, Kondor and Trivedi \cite{kondor2018generalization} proved that the incorporation of convolution functions is both necessary and sufficient for the layers of a neural network $y = f(x)$ to be equivariant.
On the other hand, when viewed from a probabilistic point of view, Bloem-Reddy and Teh \cite{bloemreddy2019probabilistic} provided necessary and sufficient conditions for a neural network $Y = f(X, \eta)$ to be invariant or equivariant under the actions of a compact group.

The conditions posed by Bloem-Reddy and Teh on the structure of $Y = f(X, \eta)$ are quite general and do not, by themselves, restrict $f$ to be a function involving convolution, as is the case in Kondor and Trivedi's results.
Then, a natural question to ask is: what further conditions are necessary on $Y = f(X, \eta)$ so that $f$ is restricted to functions involving convolution?
%%%%%%%%%%%%%%%%%%%
\section{Technical aspects}

The project will draw on technical aspects of the following areas: probabilistic symmetry, machine learning, group theory, fourier analysis, and representation theory.


%%%%%%%%%%%%%%%%%%%
\section{Literature}

The key references for this project are \cite{kondor2018generalization} and \cite{bloemreddy2019probabilistic}, as stated previously.


%%%%%%%%%%%%%%%%%%%
\section{Plan}

I will carry out this project with the following sequence of steps: 
\begin{enumerate}
  \item I will summarize the key points from \cite{bloemreddy2019probabilistic} and \cite{kondor2018generalization}, and provide links between the concepts discussed in both papers.
  \item I will attempt to derive necessary conditions on $Y = f(X, \eta)$ so that $f$ is a function involving a convolution operator.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%
\section{Why I'm interested in this topic}

I am interested in the mathematical foundations of machine learning, and I'm particularly excited in understanding how group theory, a topic I learned in my undergraduate degree in mathematics, relates to statistical learning theory.


%%%%%%%%%%%%%%%%%%%
\printbibliography


\end{document}

